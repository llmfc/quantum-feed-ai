**This file consists of the entire unaltered output for reference.**



Okay, I understand the requirements. I'll integrate the user's requests, taking into account the existing code, previous explanations, and suggested improvements. I will focus on styling the application to resemble ChatGPT and enabling the user to select specific LLMs. I will also address the known caveats and ensure the code is fully functional and deployable.

Here's the complete code for the updated application:

```json
{
  "llm_chat_app/backend/app.py": "import os\nimport json\nimport logging\nimport time\nfrom flask import Flask, request, jsonify, Response\nfrom flask_cors import CORS\nimport openai\nimport asyncio\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\napp = Flask(__name__)\nCORS(app)\n\n# Load API keys from environment variables\nOPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\nGEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\") # Placeholder, not used until Gemini integration\n\nif not OPENAI_API_KEY:\n    logging.error(\"OpenAI API key not found in environment variables.\")\n    raise ValueError(\"OpenAI API key not found in environment variables.\")\n\nopenai.api_key = OPENAI_API_KEY\n\n# In-memory context storage (for demonstration purposes)\nconversation_history = {}  # Session ID -> list of (prompt, response) tuples\n\n# --- LLM Abstraction Layer ---\nclass LLMProvider:\n    def __init__(self, name):\n        self.name = name\n\n    async def generate_response(self, prompt, context, temperature, model):\n        raise NotImplementedError\n\nclass OpenAIProvider(LLMProvider):\n    def __init__(self):\n        super().__init__(\"OpenAI\")\n\n    async def generate_response(self, prompt, context, temperature, model):\n        try:\n            messages = [{\n                \"role\": \"system\", \n                \"content\": \"You are a helpful assistant.\"\n            }]\n            # Include context in the messages\n            for user_prompt, assistant_response in context:\n                messages.append({\"role\": \"user\", \"content\": user_prompt})\n                messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n            messages.append({\"role\": \"user\", \"content\": prompt})\n\n            logging.info(f\"Calling OpenAI API with prompt: {prompt}, model: {model}\")\n\n            response = await openai.ChatCompletion.acreate(\n                model=model,  # Use selected model\n                messages=messages,\n                temperature=temperature,\n                stream=True\n            )\n            return response\n        except openai.error.RateLimitError as e:\n            logging.warning(f\"OpenAI RateLimitError: {e}\")\n            raise Exception(\"Rate limit exceeded. Please try again later.\") from e\n        except openai.error.AuthenticationError as e:\n            logging.error(f\"OpenAI AuthenticationError: {e}\")\n            raise Exception(\"Invalid OpenAI API key.\") from e\n        except openai.error.APIConnectionError as e:\n            logging.error(f\"OpenAI APIConnectionError: {e}\")\n            raise Exception(\"Failed to connect to OpenAI API.\") from e\n        except openai.error.InvalidRequestError as e:\n            logging.error(f\"OpenAI InvalidRequestError: {e}\")\n            raise Exception(f\"Invalid request to OpenAI API: {e}\")\n        except Exception as e:\n            logging.exception(\"An unexpected error occurred while calling OpenAI API:\")\n            raise Exception(f\"OpenAI API Error: {e}\") from e\n\n\nclass GeminiProvider(LLMProvider):  # Stub Implementation\n    def __init__(self):\n        super().__init__(\"Gemini\")\n\n    async def generate_response(self, prompt, context, temperature, model):\n        # Placeholder response\n        await asyncio.sleep(1)  # Simulate some processing time\n        return \"Gemini API Stub Response\"\n\n\nllm_providers = {\n    \"OpenAI\": OpenAIProvider(),\n    \"Gemini\": GeminiProvider(),\n}\n\n# --- API Endpoints ---\n@app.route(\"/chat\", methods=[\"POST\"])\nasync def chat():\n    data = request.get_json()\n    prompt = data.get(\"prompt\")\n    llm_name = data.get(\"llm\", \"OpenAI\")  # Default to OpenAI\n    temperature = data.get(\"temperature\", 0.7)  # Default temperature\n    model = data.get(\"model\", \"gpt-3.5-turbo\") # Default model\n    session_id = request.headers.get(\"Session-Id\") # Get session id from header\n\n    if not prompt:\n        return jsonify({\"error\": \"Prompt is required\"}), 400\n\n    if llm_name not in llm_providers:\n        return jsonify({\"error\": \"Invalid LLM provider\"}), 400\n\n    if not session_id:\n        return jsonify({\"error\": \"Session ID is required\"}), 400\n\n    # Get or create conversation history for the session\n    if session_id not in conversation_history:\n        conversation_history[session_id] = []\n\n    context = conversation_history[session_id]\n\n    full_response = \"\" # Accumulate the streamed response\n    start_time = time.time()\n    try:\n        llm = llm_providers[llm_name]\n\n        async def generate():\n            nonlocal full_response\n            try:\n                response = await llm.generate_response(prompt, context, temperature, model)\n                async for chunk in response:\n                    if chunk and 'choices' in chunk and len(chunk['choices']) > 0 and 'delta' in chunk['choices'][0] and 'content' in chunk['choices'][0]['delta']:\n                      chunk_message = chunk['choices'][0]['delta']['content']\n                      full_response += chunk_message  # Accumulate the response\n                      yield f\"data: {json.dumps({'content': chunk_message})}\\\\\\\n\\\\\\\n\\\"\"\n                    else:\n                      #Handle edge case where the content is empty\n                      pass\n            except Exception as e:\n                error_message = str(e)\n                logging.error(f\"Error during generation: {error_message}\")\n                yield f\"data: {json.dumps({'error': error_message})}\\\\\\\n\\\\\\\n\\\"\"\n            finally:\n                yield \"data: [DONE]\\\\\\\n\\\\\\\n\\\"\"\n\n        response =  Response(generate(), mimetype='text/event-stream')\n        logging.info(f\"Response stream started for session {session_id}\")\n\n    except Exception as e:\n        error_message = str(e)\n        logging.error(f\"Error processing chat request: {error_message}\")\n        return jsonify({\"error\": error_message}), 500\n\n    finally:\n        end_time = time.time()\n        logging.info(f\"Full response received in {end_time - start_time:.2f} seconds\")\n        # Update conversation history *after* the stream is complete\n        if 'session_id' in locals():\n          conversation_history[session_id].append((prompt, full_response))\n          conversation_history[session_id] = conversation_history[session_id][-5:] #Keep last 5 turns\n          logging.info(f\"Conversation history updated for session {session_id}\")\n\n    return response\n\n\n@app.route(\"/clear_context\", methods=[\"POST\"])\ndef clear_context():\n    session_id = request.headers.get(\"Session-Id\")\n    if session_id in conversation_history:\n        del conversation_history[session_id]\n        logging.info(f\"Context cleared for session {session_id}\")\n        return jsonify({\"message\": \"Context cleared\"})\n    else:\n        return jsonify({\"message\": \"No context to clear\"})\n\n\nif __name__ == \"__main__\":\n    app.run(debug=True, port=5000)",
  "llm_chat_app/backend/tests/test_app.py": "import pytest\nimport json\nfrom unittest.mock import AsyncMock, MagicMock\nfrom backend.app import app\n\n@pytest.fixture\ndef client():\n    app.config['TESTING'] = True\n    with app.test_client() as client:\n        yield client\n\n\n@pytest.fixture\ndef mock_openai_response():\n    # Simulate a streaming response with multiple chunks\n    def generate_chunks():\n        yield {'choices': [{'delta': {'content': 'Mocked '}}]}\n        yield {'choices': [{'delta': {'content': 'OpenAI '}}]}\n        yield {'choices': [{'delta': {'content': 'response'}}]}\n\n    return generate_chunks()\n\n\n@pytest.mark.asyncio\nasync def test_chat_endpoint_no_prompt(client):\n    response = client.post('/chat', json={}, headers={'Session-Id': 'test_session'})\n    assert response.status_code == 400\n    data = json.loads(response.data.decode('utf-8'))\n    assert data['error'] == \"Prompt is required\"\n\n\n@pytest.mark.asyncio\nasync def test_chat_endpoint_openai(client, mocker, mock_openai_response):\n    mocker.patch('backend.app.openai.ChatCompletion.acreate', new_callable=AsyncMock, return_value=mock_openai_response)\n\n    response = client.post(\n        '/chat',\n        json={'prompt': 'Test prompt', 'llm': 'OpenAI', 'model': 'gpt-3.5-turbo'},\n        headers={'Session-Id': 'test_session'}\n    )\n\n    assert response.status_code == 200\n    # Simulate reading the stream\n    full_response = \"\"\n    for line in response.iter_lines():\n        if line:\n            decoded_line = line.decode('utf-8')\n            if decoded_line.startswith(\"data:\"):\n                data = json.loads(decoded_line[5:])\n                if 'content' in data:\n                    full_response += data['content']\n\n    assert \"Mocked OpenAI response\" in full_response\n\n\n\n@pytest.mark.asyncio\nasync def test_chat_endpoint_gemini_stub(client):\n    response = client.post(\n        '/chat',\n        json={'prompt': 'Test prompt', 'llm': 'Gemini', 'model': 'gemini-pro'},\n        headers={'Session-Id': 'test_session'}\n    )\n    assert response.status_code == 200\n    # Simulate reading the stream\n    full_response = \"\"\n    for line in response.iter_lines():\n        if line:\n            decoded_line = line.decode('utf-8')\n            if decoded_line.startswith(\"data:\"):\n                data = json.loads(decoded_line[5:])\n                if 'content' in data:\n                    full_response += data['content']\n    assert \"Gemini API Stub Response\" in full_response\n\n\n@pytest.mark.asyncio\nasync def test_clear_context(client):\n    # First, create some context\n    client.post(\n        '/chat',\n        json={'prompt': 'Test prompt', 'llm': 'OpenAI', 'model': 'gpt-3.5-turbo'},\n        headers={'Session-Id': 'test_session'}\n    )\n    response = client.post('/clear_context', headers={'Session-Id': 'test_session'})\n    assert response.status_code == 200\n    data = json.loads(response.data.decode('utf-8'))\n    assert data['message'] == \"Context cleared\"",
  "llm_chat_app/backend/Dockerfile": "FROM python:3.9-slim-buster\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [\"python\", \"app.py\"]",
  "llm_chat_app/backend/requirements.txt": "Flask==2.3.2\nFlask-CORS==3.0.10\nopenai==0.27.7\npython-dotenv==1.0.0\nrequests==2.31.0\npytest\npytest-mock\npytest-asyncio",
  "llm_chat_app/backend/.env": "OPENAI_API_KEY=YOUR_OPENAI_API_KEY\nGEMINI_API_KEY=YOUR_GEMINI_API_KEY # Optional, if you have a Gemini API key",
  "llm_chat_app/frontend/src/App.js": "import React, { useState, useEffect, useRef } from 'react';\nimport './App.css';\n\nfunction App() {\n    const [prompt, setPrompt] = useState('');\n    const [chatHistory, setChatHistory] = useState([]);\n    const [llm, setLlm] = useState('OpenAI');\n    const [model, setModel] = useState('gpt-3.5-turbo'); // Default model\n    const [temperature, setTemperature] = useState(0.7);\n    const [sessionId, setSessionId] = useState(null);\n    const [isStreaming, setIsStreaming] = useState(false);\n    const [currentStreamedResponse, setCurrentStreamedResponse] = useState('');\n    const chatHistoryRef = useRef(null);\n    const [isLoading, setIsLoading] = useState(false);\n\n    const llmModels = {\n        'OpenAI': [\n            'gpt-3.5-turbo',\n            'gpt-4',\n            'gpt-4-turbo-preview'\n        ],\n        'Gemini': [\n            'gemini-pro',\n            'gemini-1.5-pro-latest'\n        ],\n        'Mistral': [\n            'mistral-tiny',\n            'mistral-small',\n            'mistral-medium'\n        ],\n        'O1': [\n            'oasst-sft-1-pythia-12b',\n            'oasst-sft-6-llama-30b'\n        ],\n        'LLama': [\n            'Llama-2-7b-chat-hf',\n            'Llama-2-13b-chat-hf'\n        ]\n    };\n\n    useEffect(() => {\n        const storedChatHistory = localStorage.getItem('chatHistory');\n        if (storedChatHistory) {\n            setChatHistory(JSON.parse(storedChatHistory));\n        }\n\n        let session = localStorage.getItem('sessionId');\n        if (!session) {\n            session = generateSessionId();\n            localStorage.setItem('sessionId', session);\n        }\n        setSessionId(session);\n    }, []);\n\n    useEffect(() => {\n        localStorage.setItem('chatHistory', JSON.stringify(chatHistory));\n    }, [chatHistory]);\n\n    const generateSessionId = () => {\n        return Math.random().toString(36).substring(2, 15);\n    };\n\n    const handleLlmChange = (event) => {\n        const selectedLlm = event.target.value;\n        setLlm(selectedLlm);\n        // Reset model to the first option when LLM changes\n        setModel(llmModels[selectedLlm][0]);\n    };\n\n    const handleModelChange = (event) => {\n        setModel(event.target.value);\n    };\n\n    const handleTemperatureChange = (event) => {\n        setTemperature(parseFloat(event.target.value));\n    };\n\n    useEffect(() => {\n        if (chatHistoryRef.current) {\n            chatHistoryRef.current.scrollTop = chatHistoryRef.current.scrollHeight;\n        }\n    }, [chatHistory]);\n\n    const handleSubmit = async (event) => {\n        event.preventDefault();\n\n        if (!prompt.trim()) {\n            return;\n        }\n\n        setIsStreaming(true);\n        setCurrentStreamedResponse('');\n        setIsLoading(true);\n\n        const newChatHistory = [...chatHistory, { type: 'user', text: prompt }];\n        setChatHistory(newChatHistory);\n\n        try {\n            const response = await fetch('http://localhost:5000/chat', {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json',\n                    'Accept': 'text/event-stream',\n                    'Session-Id': sessionId,\n                },\n                body: JSON.stringify({ prompt: prompt, llm: llm, temperature: temperature, model: model }),\n                credentials: 'omit',\n            });\n\n            if (!response.ok) {\n                const errorData = await response.json();\n                throw new Error(errorData?.error || `HTTP error! Status: ${response.status}`);\n            }\n\n            const reader = response.body.getReader();\n            const decoder = new TextDecoder();\n            let buffer = '';\n\n            while (true) {\n                const { done, value } = await reader.read();\n\n                if (done) {\n                    break;\n                }\n\n                buffer += decoder.decode(value);\n\n                let eventEndIndex;\n                while ((eventEndIndex = buffer.indexOf('\\n\\n')) !== -1) {\n                    const event = buffer.substring(0, eventEndIndex).trim();\n                    buffer = buffer.substring(eventEndIndex + 2);\n\n                    if (event.startsWith('data:')) {\n                        const data = event.substring(5).trim();\n                        if (data === '[DONE]') {\n                            setIsStreaming(false);\n                            break;\n                        }\n\n                        try {\n                            const parsedData = JSON.parse(data);\n                            if (parsedData.content) {\n                                setCurrentStreamedResponse((prev) => prev + parsedData.content);\n                            } else if (parsedData.error) {\n                                console.error(\"Error from backend:\", parsedData.error);\n                                setChatHistory(prev => [...prev, { type: 'error', text: `Error: ${parsedData.error}` }]);\n                                setIsStreaming(false);\n                                reader.cancel();\n                                break;\n                            }\n                        } catch (parseError) {\n                            console.error(\"Error parsing SSE data:\", parseError, \"Data:\", data);\n                        }\n                    }\n                }\n            }\n        } catch (error) {\n            console.error('Fetch error:', error);\n            setChatHistory(prev => [...prev, { type: 'error', text: `Error: ${error.message}` }]);\n            setIsStreaming(false);\n        } finally {\n            setIsStreaming(false);\n            setIsLoading(false);\n        }\n\n        setPrompt('');\n    };\n\n    useEffect(() => {\n        if (!isStreaming && currentStreamedResponse) {\n            setChatHistory(prev => [...prev, { type: 'bot', text: currentStreamedResponse }]);\n        }\n    }, [isStreaming, currentStreamedResponse]);\n\n    const handleClearContext = async () => {\n        try {\n            const response = await fetch('http://localhost:5000/clear_context', {\n                method: 'POST',\n                headers: {\n                    'Content-Type': 'application/json',\n                    'Session-Id': sessionId,\n                },\n                credentials: 'omit',\n            });\n\n            if (response.ok) {\n                setChatHistory([]);\n                localStorage.removeItem('chatHistory');\n            } else {\n                console.error('Failed to clear context');\n            }\n        } catch (error) {\n            console.error('Error clearing context:', error);\n        }\n    };\n\n    return (\n        <div className=\"App\">\n            <h1 className=\"app-title\">LLM Chat Application</h1>\n            <div className=\"model-options\">\n                <div className=\"llm-selector\">\n                    <label htmlFor=\"llm\">LLM:</label>\n                    <select id=\"llm\" value={llm} onChange={handleLlmChange}>\n                        {Object.keys(llmModels).map((llmName) => (\n                            <option key={llmName} value={llmName}>{llmName}</option>\n                        ))}\n                    </select>\n                </div>\n\n                <div className=\"model-selector\">\n                    <label htmlFor=\"model\">Model:</label>\n                    <select id=\"model\" value={model} onChange={handleModelChange}>\n                        {llmModels[llm].map((modelName) => (\n                            <option key={modelName} value={modelName}>{modelName}</option>\n                        ))}\n                    </select>\n                </div>\n            </div>\n\n            <div className=\"chat-container\">\n                <div className=\"chat-history\" ref={chatHistoryRef}>\n                    {chatHistory.map((message, index) => (\n                        <div key={index} className={`message ${message.type}`}>\n                            <div className=\"message-content\">\n                                {message.text}\n                            </div>\n                        </div>\n                    ))}\n                    {isStreaming && (\n                        <div className=\"message bot\">\n                            <div className=\"message-content\">\n                                {currentStreamedResponse}\n                            </div>\n                        </div>\n                    )}\n                    {isLoading && <div className=\"loading-spinner\">Loading...</div>}\n                </div>\n                <form onSubmit={handleSubmit} className=\"prompt-input\">\n                    <input\n                        type=\"text\"\n                        value={prompt}\n                        onChange={(e) => setPrompt(e.target.value)}\n                        placeholder=\"Enter your prompt...\"\n                        disabled={isStreaming}\n                    />\n                    <button type=\"submit\" disabled={isStreaming || isLoading}>\n                        {isStreaming || isLoading ? 'Sending...' : 'Send'}\n                    </button>\n                </form>\n                <div className=\"controls\">\n                    <div className=\"temperature-control\">\n                        <label htmlFor=\"temperature\">Temperature:</label>\n                        <input\n                            type=\"range\"\n                            id=\"temperature\"\n                            min=\"0\"\n                            max=\"1\"\n                            step=\"0.01\"\n                            value={temperature}\n                            onChange={handleTemperatureChange}\n                        />\n                        <span>{temperature}</span>\n                    </div>\n                    <button onClick={handleClearContext} className=\"new-chat-button\">New Chat</button>\n                </div>\n            </div>\n        </div>\n    );\n}\n\nexport default App;",
  "llm_chat_app/frontend/src/App.css": ".App {\n    font-family: sans-serif;\n    text-align: center;\n    padding: 20px;\n    background-color: #343541; /* Dark background like ChatGPT */\n    color: #fff; /* Light text color */\n    height: 100vh;\n    display: flex;\n    flex-direction: column;\n    align-items: center;\n}\n\n.app-title {\n    margin-bottom: 20px;\n    font-size: 2em;\n    color: #fff;\n}\n\n.model-options {\n    display: flex;\n    justify-content: center;\n    gap: 20px;\n    margin-bottom: 10px;\n    width: 100%;\n}\n\n.llm-selector, .model-selector {\n    display: flex;\n    flex-direction: column;\n    align-items: center;\n    color: #fff;\n}\n\nlabel {\n    margin-bottom: 5px;\n    color: #fff;\n}\n\nselect {\n    padding: 8px;\n    border-radius: 5px;\n    border: none;\n    background-color: #444654; /* Darker select background */\n    color: #fff;\n}\n\n.chat-container {\n    width: 80%;\n    max-width: 800px;\n    flex-grow: 1;\n    display: flex;\n    flex-direction: column;\n    margin-bottom: 20px;\n}\n\n.chat-history {\n    border: 1px solid #555;\n    padding: 10px;\n    margin-bottom: 10px;\n    height: 300px;\n    overflow-y: scroll;\n    text-align: left;\n    position: relative;\n    background-color: #444654; /* Dark chat history background */\n    border-radius: 5px;\n    color: #fff;\n}\n\n.message {\n    padding: 8px;\n    margin-bottom: 5px;\n    border-radius: 5px;\n    word-wrap: break-word;\n}\n\n.message.user {\n    background-color: #343541; /* Dark user message background */\n    text-align: right;\n    color: #fff;\n}\n\n.message.bot {\n    background-color: #434654; /* Darker bot message background */\n    text-align: left;\n    color: #fff;\n}\n\n.message.error {\n    background-color: #8B0000; /* Dark red for error messages */\n    color: #fff;\n    text-align: left;\n}\n\n.message-content {\n    white-space: pre-wrap;\n}\n\n.prompt-input {\n    display: flex;\n    margin-bottom: 10px;\n}\n\n.prompt-input input[type=\"text\"] {\n    width: 70%;\n    padding: 8px;\n    margin-right: 10px;\n    border-radius: 5px;\n    border: none;\n    background-color: #555;\n    color: #fff;\n}\n\n.prompt-input button {\n    padding: 8px 12px;\n    cursor: pointer;\n    border-radius: 5px;\n    border: none;\n    background-color: #50a050; /* Green send button */\n    color: #fff;\n}\n\n.prompt-input button:disabled {\n    background-color: #777;\n    cursor: not-allowed;\n}\n\n.controls {\n    display: flex;\n    justify-content: space-between;\n    align-items: center;\n    margin-bottom: 10px;\n}\n\n.temperature-control {\n    display: flex;\n    flex-direction: column;\n    align-items: center;\n    color: #fff;\n}\n\n.temperature-control input[type=\"range\"] {\n    width: 150px;\n}\n\n.new-chat-button {\n    padding: 8px 12px;\n    cursor: pointer;\n    border-radius: 5px;\n    border: none;\n    background-color: #3d85c6; /* Blue new chat button */\n    color: #fff;\n}\n\n.loading-spinner {\n    position: absolute;\n    bottom: 10px;\n    left: 50%;\n    transform: translateX(-50%);\n    font-style: italic;\n    color: #888;\n}",
  "llm_chat_app/frontend/src/index.js": "import React from 'react';\nimport ReactDOM from 'react-dom/client';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nconst root = ReactDOM.createRoot(document.getElementById('root'));\nroot.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();",
  "llm_chat_app/frontend/src/index.css": "body {\n    margin: 0;\n    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',\n        'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',\n        sans-serif;\n    -webkit-font-smoothing: antialiased;\n    -moz-osx-font-smoothing: grayscale;\n    background-color: #343541; /* Match app background */\n    color: #fff; /* Set default text color */\n}",
  "llm_chat_app/frontend/src/setupTests.js": "// jest-dom adds custom jest matchers for asserting on DOM nodes.\n// allows you to do things like:\n// expect(element).toHaveTextContent(/react/i)\n// learn more: https://github.com/testing-library/jest-dom\nimport '@testing-library/jest-dom';",
  "llm_chat_app/frontend/public/index.html": "<!DOCTYPE html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"utf-8\" />\n    <link rel=\"icon\" href=\"%PUBLIC_URL%/favicon.ico\" />\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n    <meta name=\"theme-color\" content=\"#000000\" />\n    <meta\n      name=\"description\"\n      content=\"Web app for LLM Chat\"\n    />\n    <link rel=\"apple-touch-icon\" href=\"%PUBLIC_URL%/logo192.png\" />\n    <!--\n      manifest.json provides metadata used when your web app is installed on a\n      user's mobile device or desktop. See https://developers.google.com/web/fundamentals/web-app-manifest/\n    -->\n    <link rel=\"manifest\" href=\"%PUBLIC_URL%/manifest.json\" />\n    <!--\n      Notice the use of %PUBLIC_URL% in the tags above.\n      It will be replaced with the URL of the `public` folder during the build.\n      Only files inside the `public` folder can be referenced from the HTML.\n\n      Unlike \"/favicon.ico\" or \"favicon.ico\", \"%PUBLIC_URL%/favicon.ico\" will\n      work correctly both with client-side routing and a non-root public URL.\n    -->\n    <title>LLM Chat App</title>\n  </head>\n  <body>\n    <noscript>You need to enable JavaScript to run this app.</noscript>\n    <div id=\"root\"></div>\n    <!--\n      This HTML file is a template.\n      If you open it directly in the browser, you will see an empty page.\n\n      You can add webfonts, meta tags, or analytics to this file.\n      The build step will place the bundled scripts into the <body> tag.\n\n      To begin the development, run `npm start` or `yarn start`.\n      To create a production bundle, use `npm run build` or `yarn build`.\n    -->\n  </body>\n</html>",
  "llm_chat_app/frontend/Dockerfile": "FROM node:16-alpine as builder\n\nWORKDIR /app\nCOPY package*.json ./\nRUN npm install\nCOPY . .\nRUN npm run build\n\nFROM nginx:alpine\n\nCOPY --from=builder /app/build /usr/share/nginx/html\nEXPOSE 80\nCMD [\"nginx\", \"-g\", \"daemon off;\"]",
  "llm_chat_app/frontend/package.json": "{\n  \"name\": \"frontend\",\n  \"version\": \"0.1.0\",\n  \"private\": true,\n  \"dependencies\": {\n    \"@testing-library/jest-dom\": \"^5.16.5\",\n    \"@testing-library/react\": \"^13.4.0\",\n    \"@testing-library/user-event\": \"^13.5.0\",\n    \"react\": \"^18.2.0\",\n    \"react-dom\": \"^18.2.0\",\n    \"react-scripts\": \"5.0.1\",\n    \"web-vitals\": \"^2.1.4\"\n  },\n  \"scripts\": {\n    \"start\": \"react-scripts start\",\n    \"build\": \"react-scripts build\",\n    \"test\": \"react-scripts test\",\n    \"eject\": \"react-scripts eject\"\n  },\n  \"eslintConfig\": {\n    \"extends\": [\n      \"react-app\",\n      \"react-app/jest\"\n    ]\n  },\n  \"browserslist\": {\n    \"production\": [\n      \">0.2%\",\n      \"not dead\",\n      \"not op_mini all\"\n    ],\n    \"development\": [\n      \"last 1 chrome version\",\n      \"last 1 firefox version\",\n      \"last 1 safari version\"\n    ]\n  },\n  \"devDependencies\": {}\n}",
  "llm_chat_app/docker-compose.yml": "version: \"3.8\"\nservices:\n  frontend:\n    build:\n      context: ./frontend\n      dockerfile: Dockerfile\n    ports:\n      - \"3000:80\"\n    depends_on:\n      - backend\n    environment:\n      - REACT_APP_BACKEND_URL=http://localhost:5000\n\n  backend:\n    build:\n      context: ./backend\n      dockerfile: Dockerfile